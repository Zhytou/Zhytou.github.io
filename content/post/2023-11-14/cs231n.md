---
title: "CS231n笔记"
date: 2023-11-14T09:58:05+08:00
draft: fasle
---

这篇博客流水线的记录一下自己学习[CS231n](http://cs231n.stanford.edu/schedule.html)的笔记。

## 1 Computer Vision Overview

![图1 CV概述](./cv_overview.PNG)

## 2 Image Classification with Linear Classifiers

### The image classification

图像分类是CV领域中最核心任务之一。它指的是，给定一些标签并将图片预测为一个或多个预定义类别中的过程。

一些潜在的挑战包括：

- Viewpoint variation
- Illumination
- Background Clutter
- Occlusion
- Deformation

### Machine Learning: Data-Driven Approach

使用机器学习解决图像分类的流程是：

- 收集数据并定义标签；
- 使用机器学习算法训练分类器；
- 在测试集上评估分类器的准确率。

### Nearest Neighbor

邻近算法的核心思想就是:

- 确定测试实例与每个训练实例的距离(相似程度);
- 从中选择距离最小的那个训练实例，称为"最近的邻居";
- 将测试实例预测为这个"最近邻居"的类别。

值得注意的是，近邻算法是"惰性学习"模型家族的一部分，这意味着它不会根据训练集主动学习或者拟合出一个函数来对新进入的样本进行判断，而是单纯的记住训练集中所有的样本，所以它实际上没有所谓的"训练"过程，而是在需要进行预测的时候从自己的训练集样本中查找与新进入样本最相似的样本，即寻找最近邻来获得预测结果。

**Distance Metric**：

一些常见的距离度量方法包括：

- L1 Manhattan distance: $d_{L1}(x, y) = \sum_{i=1}^n |x_i - y_i|$
- L2 Euclidean distance: $d_{L2}(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}$

**K Nearest Neighbor**：

k邻近算法是使用离待测试点k个最近邻的点来对分类进行预测的一种算法。相比普通最近邻算法只考虑测试实例的单一最近邻居，而k邻域算法考虑测试实例的k个最近邻居。

对于分类问题，k邻域算法通过majority voting的方式从k个最近邻居中获得最常见的类别作为预测类别，即：选择k个邻居中频率最高的类别作为返回值。

**Hyperparameters**：

在机器学习中，超参数是在开始学习过程之前设置用于控制学习过程的参数，而不是通过训练得到的参数数据。

**Cross-Validation**：

交叉验证在训练集和测试集的基础上，通过进一步将训练数据集随机平均划分为K个互不重叠的子集，然后将其中一个子集作为验证集，其余K-1个子集作为临时训练集。这样循环K次，每次选择一个不同的验证集，其余作为临时训练集，取平均值作为最终估计值。

因为交叉验证可以重复利用原始数据集，较好地评估不同超参数设置下的泛化能力，所以它一般常用来确认和选择模型的超参数。

### Linear Classifier

![图2 线性分类器](./linear_model.PNG)

线性分类的方法一般由2个部分组成：决策函数和损失函数。

**Score Function**：

决策函数用于对样本进行评分，得到每个类的得分。比如：最简单的线性分类器使用函数$f(x) = w^T x + b$

**How To Choose A Good Linear Classifier**：

- 定义一个损失函数，用于反映模型预测效果；
- 优化模型参数来减小损失函数的值。

**Loss Function**：

损失函数一般是每个样本的平均预测值与真实值的差值，比如：$L = \frac{1}{N} \sum_{i} L_i(f_i(x_i, W), y)$

### Multiclass SVM Loss

### Softmax Classifier

> 回归 vs 分类：定量输出称为回归，或者说是连续变量预测； 定性输出称为分类，或者说是离散变量预测。

Softmax的作用就是将分类器输出的数值转化为一个概率分布。具体来说：普通分类器的最后一层为线性层，输出为每个类的得分。而Softmax函数将这些得分映射到(0,1)范围内，且保证各类得分和为1。

## 3 Regularization and Optimization

### Regularization

通过在目标函数中添加一个“惩罚项”,来限制模型参数的大小或复杂度。

$L(W) = \frac{1}{N} \sum_{i} L_i(f_i(x_i, W), y) + \lambda R(W)$

**Why regularize?**：

- Express preferences over weights
- Make the model simple so it works on test data
- Improve optimization by adding curvature

### Optimization

**Gradient**:

梯度是一个向量，每个元素表示某个变量对目标函数的变化影响程度，即：偏导数。对于目标函数F(w)来说，它的梯度可以表示为∇F(w)。

梯度向量指出目标函数在各个方向上增长最快的方向

## 4 Neural Networks and Backpropagation

### Neural Networks

神经网络通常也被称为全连接网络（Fully Connected Network），它指的是神经元层与层之间都是完全连接的，一个层的每个节点都会连接到下一层的每个节点的结构。

一个最基础的两层神经网络可以由线性模型变化而来，比如：

- Linear function: $f = Wx$
- 2-layer Neural Networks: $f = W_2max(0, W_1x)$

**Activation functions**：

上面的两层神经网络表达式中的max函数也被称为激活函数。它是神经网络区别与普通模型的关键，如果缺少这个函数，上面的模型就变成$f = W_2W_1x$。

一些常见的激活函数包括：

- Sigmoid
- tanh
- ReLU
- Leaky ReLU
- Maxout
- ELU

### Backpropagation

## 5 Image Classification with CNNs

### Full Connected Layer

全连接层

### Convotional Layer

### Pooling Layer

## 6 CNN Architectures
